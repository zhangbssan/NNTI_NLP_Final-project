{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MODEL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdiD45oNZx6X"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "torch.manual_seed(10)\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from matplotlib import pyplot as plt\n",
        "import nltk\n",
        "import torch.nn as  nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tDGz9xzDWMc"
      },
      "source": [
        "Build the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VIwezGDqSgJ"
      },
      "source": [
        "def vocabulary(data):\n",
        "  V = {}\n",
        "  i=0\n",
        "  for s in range(len(data)):\n",
        "    n=data[s]\n",
        "    for y in range(len(n)):\n",
        "      w=data[s][y]\n",
        "      if w not in V:\n",
        "        V[w] = i\n",
        "        i+=1\n",
        "      y+=1\n",
        "    s+=1\n",
        "  return V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBgPfsxKYMlz"
      },
      "source": [
        "Skip-Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L19vWo3fW7E_"
      },
      "source": [
        "def get_target_context(sentence):\n",
        "    word_lists=[]\n",
        "    for i in range(len(sentence)):\n",
        "       w=sentence[i]\n",
        "       for n in range(2):\n",
        "                # look back\n",
        "          if (i-n-1)>=0:\n",
        "            word_lists.append([w] + [sentence[i-n-1]])\n",
        "                \n",
        "                # look forward\n",
        "          if (i+n+1)<len(sentence):\n",
        "            word_lists.append([w]+[sentence[i+n+1]])\n",
        "    return word_lists\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hShnmPSKYJ9X"
      },
      "source": [
        " Pytorch Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJeNdVT1Xr2L"
      },
      "source": [
        "embedding_size = 64\n",
        "vocabulary_size=4882 # for Bengali\n",
        "vocabulary_size1=11137 # for Hindi\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_size, vocab_size):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.input = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.output = nn.Linear(embedding_size, vocab_size,bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, one_hot):\n",
        "        #one_hot = torch.tensor(one_hotï¼‰\n",
        "        \n",
        "        emb = self.input(one_hot)\n",
        "        hidden = self.output(emb)\n",
        "        out = F.log_softmax(hidden)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7x1HKQUPOwk"
      },
      "source": [
        "EMBEDDING_DIM = embedding_size \n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [2,3,4]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "       # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
        "        \n",
        "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
        "        \n",
        "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "                \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = text\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "        \n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxSwjQoKW38x"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L-mEVletqXc"
      },
      "source": [
        "import copy\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z1JokNFtrqn"
      },
      "source": [
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(query, key, value, mask=mask,\n",
        "                                 dropout=self.dropout)\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(nbatches, -1, self.h * self.d_k)\n",
        "        x = torch.squeeze(x)\n",
        "        return self.linears[-1](x)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud20wzkdt9Q9"
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYDhYVS4uDGq"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # mean = torch.Tensor(x.mean(-1, keepdims=True)).to('cuda')\n",
        "        # std = torch.Tensor(x.std(-1, keepdims=True)).to('cuda')\n",
        "        # x = torch.Tensor(x).to('cuda')\n",
        "        mean = x.mean(-1,keepdim=True)\n",
        "        std = x.std(-1,keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity we apply the norm first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer function that maintains the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    #Encoder is made up of two sublayers, self-attn and feed forward (defined below)\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpuOUoT3uNJI"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \n",
        "\n",
        "    def __init__(self, encoder, src_embed, d_model, n_class):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.src_embed = src_embed\n",
        "        self.linear = nn.Linear(d_model, n_class)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        enc = self.encoder(src, src_mask)\n",
        "        # flat = enc.reshape(-1).unsqueeze(0)\n",
        "        lin = self.linear(enc)\n",
        "        result = F.softmax(lin, dim=1)\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ns6UK9TuUbI"
      },
      "source": [
        "SRC_VOCAB=1\n",
        "N_CLASS=1\n",
        "D_MODEL=embedding_size\n",
        "D_FF=1024\n",
        "N = 6\n",
        "H=8\n",
        "DROP_OUT=0.1\n",
        "\n",
        "def make_model(src_vocab, N,\n",
        "               d_model, d_ff, h, dropout, n_class):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    #position = PositionalEncoding(d_model, dropout)\n",
        "    model = TransformerEncoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        src_vocab,\n",
        "        d_model,\n",
        "        n_class\n",
        "    )\n",
        "\n",
        "    # This was important from their code.\n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}